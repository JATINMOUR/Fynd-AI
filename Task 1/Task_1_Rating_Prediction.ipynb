{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhSwkoommVSK",
        "outputId": "4efbcdc4-64bf-48a0-f48f-1dcefa93d477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating V1: Direct ---\n",
            "Processed 10/200\n",
            "Processed 20/200\n",
            "Processed 30/200\n",
            "Processed 40/200\n",
            "Processed 50/200\n",
            "Processed 60/200\n",
            "Processed 70/200\n",
            "Processed 80/200\n",
            "Processed 90/200\n",
            "Processed 100/200\n",
            "Processed 110/200\n",
            "Processed 120/200\n",
            "Processed 130/200\n",
            "Processed 140/200\n",
            "Processed 150/200\n",
            "Processed 160/200\n",
            "Processed 170/200\n",
            "Processed 180/200\n",
            "Processed 190/200\n",
            "Processed 200/200\n",
            "Results for V1: Direct:\n",
            " - Accuracy: 5.5%\n",
            " - +/- 1 Star Accuracy: 10.5%\n",
            " - JSON Validity: 10.5%\n",
            "\n",
            "--- Evaluating V2: Rubric ---\n",
            "Processed 10/200\n",
            "Processed 20/200\n",
            "Processed 30/200\n",
            "Processed 40/200\n",
            "Processed 50/200\n",
            "Processed 60/200\n",
            "Processed 70/200\n",
            "Processed 80/200\n",
            "Processed 90/200\n",
            "Processed 100/200\n",
            "Processed 110/200\n",
            "Processed 120/200\n",
            "Processed 130/200\n",
            "Processed 140/200\n",
            "Processed 150/200\n",
            "Processed 160/200\n",
            "Processed 170/200\n",
            "Processed 180/200\n",
            "Processed 190/200\n",
            "Processed 200/200\n",
            "Results for V2: Rubric:\n",
            " - Accuracy: 0.0%\n",
            " - +/- 1 Star Accuracy: 0.0%\n",
            " - JSON Validity: 0.0%\n",
            "\n",
            "--- Evaluating V3: CoT ---\n",
            "Processed 10/200\n",
            "Processed 20/200\n",
            "Processed 30/200\n",
            "Processed 40/200\n",
            "Processed 50/200\n",
            "Processed 60/200\n",
            "Processed 70/200\n",
            "Processed 80/200\n",
            "Processed 90/200\n",
            "Processed 100/200\n",
            "Processed 110/200\n",
            "Processed 120/200\n",
            "Processed 130/200\n",
            "Processed 140/200\n",
            "Processed 150/200\n",
            "Processed 160/200\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google import genai\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# --- Configuration & Initialization ---\n",
        "# The execution environment provides the API key automatically.\n",
        "client = genai.Client(api_key=\"AIzaSyDaLU3unG0kN_L7Iw80NsMHLIVuxERrt3c\")\n",
        "MODEL_NAME = \"gemini-2.5-flash-preview-09-2025\"\n",
        "\n",
        "def call_gemini_with_retry(prompt: str, retries: int = 5) -> str:\n",
        "    \"\"\"Calls Gemini API with exponential backoff.\"\"\"\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            response = client.models.generate_content(\n",
        "                model=MODEL_NAME,\n",
        "                contents=prompt\n",
        "            )\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            if i == retries - 1:\n",
        "                return \"ERROR: Max retries exceeded\"\n",
        "            time.sleep(2**i)\n",
        "    return \"ERROR\"\n",
        "\n",
        "# --- Prompting Strategies ---\n",
        "\n",
        "def get_prompt_v1(review_text: str) -> str:\n",
        "    \"\"\"Version 1: Direct Instruction (Baseline)\"\"\"\n",
        "    return f\"\"\"You are a sentiment analysis assistant.\n",
        "Task: Classify the following Yelp review into a rating of 1 to 5 stars.\n",
        "Output: Return only a JSON object in this format:\n",
        "{{\n",
        "\"predicted_stars\": <int>,\n",
        "\"explanation\": \"<string>\"\n",
        "}}\n",
        "\n",
        "Review: {review_text}\"\"\"\n",
        "\n",
        "def get_prompt_v2(review_text: str) -> str:\n",
        "    \"\"\"Version 2: Analytical Rubric (Improved Context)\"\"\"\n",
        "    return f\"\"\"Role: Expert Customer Experience Analyst.\n",
        "Task: Predict the star rating (1-5) for a Yelp review based on this rubric:\n",
        "- 1 Star: Disastrous, fundamental failures, \"never again.\"\n",
        "- 2 Stars: Poor experience with a few redeeming qualities.\n",
        "- 3 Stars: Average/Mediocre; met basic expectations but didn't impress.\n",
        "- 4 Stars: Great; minor issues but overall very positive.\n",
        "- 5 Stars: Exceptional; perfect service/product, highly recommended.\n",
        "\n",
        "Constraint: Respond ONLY with valid JSON.\n",
        "JSON Structure:\n",
        "{{\n",
        "\"predicted_stars\": <int>,\n",
        "\"explanation\": \"Briefly describe the specific sentiment triggers found in the text.\"\n",
        "}}\n",
        "\n",
        "Review: {review_text}\"\"\"\n",
        "\n",
        "def get_prompt_v3(review_text: str) -> str:\n",
        "    \"\"\"Version 3: Chain-of-Thought (CoT) Extraction\"\"\"\n",
        "    return f\"\"\"Analyze the provided Yelp review.\n",
        "1. Identify the key Pros mentioned.\n",
        "2. Identify the key Cons mentioned.\n",
        "3. Determine the final star rating (1-5) based on the balance of these factors.\n",
        "\n",
        "Final Output: You must provide your final answer in JSON format only.\n",
        "{{\n",
        "\"predicted_stars\": <int>,\n",
        "\"explanation\": \"A summary of the Pros/Cons balance that led to this rating.\"\n",
        "}}\n",
        "\n",
        "Review: {review_text}\"\"\"\n",
        "\n",
        "# --- Evaluation Logic ---\n",
        "\n",
        "def clean_json_response(raw_text: str) -> Dict[str, Any]:\n",
        "    \"\"\"Extracts and parses JSON from model response.\"\"\"\n",
        "    try:\n",
        "        # Remove markdown code blocks if present\n",
        "        cleaned = raw_text.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "        return json.loads(cleaned)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def run_evaluation(df: pd.DataFrame, prompt_func, version_name: str):\n",
        "    print(f\"\\n--- Evaluating {version_name} ---\")\n",
        "    results = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        prompt = prompt_func(row['text'])\n",
        "        raw_response = call_gemini_with_retry(prompt)\n",
        "        parsed = clean_json_response(raw_response)\n",
        "\n",
        "        actual = int(row['stars'])\n",
        "        predicted = parsed.get(\"predicted_stars\") if parsed else None\n",
        "\n",
        "        is_valid_json = parsed is not None\n",
        "        is_correct = (predicted == actual) if predicted is not None else False\n",
        "        is_within_one = (abs(predicted - actual) <= 1) if predicted is not None else False\n",
        "\n",
        "        results.append({\n",
        "            \"actual\": actual,\n",
        "            \"predicted\": predicted,\n",
        "            \"is_valid_json\": is_valid_json,\n",
        "            \"is_correct\": is_correct,\n",
        "            \"is_within_one\": is_within_one\n",
        "        })\n",
        "\n",
        "        # Simple progress log\n",
        "        if (idx + 1) % 10 == 0:\n",
        "            print(f\"Processed {idx + 1}/{len(df)}\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    res_df = pd.DataFrame(results)\n",
        "    accuracy = res_df['is_correct'].mean() * 100\n",
        "    within_one = res_df['is_within_one'].mean() * 100\n",
        "    json_validity = res_df['is_valid_json'].mean() * 100\n",
        "\n",
        "    print(f\"Results for {version_name}:\")\n",
        "    print(f\" - Accuracy: {accuracy:.1f}%\")\n",
        "    print(f\" - +/- 1 Star Accuracy: {within_one:.1f}%\")\n",
        "    print(f\" - JSON Validity: {json_validity:.1f}%\")\n",
        "\n",
        "    return {\n",
        "        \"version\": version_name,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"within_one\": within_one,\n",
        "        \"json_validity\": json_validity\n",
        "    }\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Note: Replace with actual path to your kaggle 'yelp.csv'\n",
        "    # For this demo, we assume the dataset has 'text' and 'stars' columns\n",
        "    try:\n",
        "        data = pd.read_csv(\"/yelp.csv\")\n",
        "        # Sample 200 rows for efficiency\n",
        "        sample_df = data.sample(n=200, random_state=42).reset_index(drop=True)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: 'yelp.csv' not found. Please ensure the dataset is in the working directory.\")\n",
        "        # Mock data for demonstration if file is missing\n",
        "        sample_df = pd.DataFrame({\n",
        "            \"text\": [\"The food was great but the service was slow.\", \"Terrible experience!\", \"Best pizza in town.\"],\n",
        "            \"stars\": [4, 1, 5]\n",
        "        })\n",
        "\n",
        "    evaluations = []\n",
        "    evaluations.append(run_evaluation(sample_df, get_prompt_v1, \"V1: Direct\"))\n",
        "    evaluations.append(run_evaluation(sample_df, get_prompt_v2, \"V2: Rubric\"))\n",
        "    evaluations.append(run_evaluation(sample_df, get_prompt_v3, \"V3: CoT\"))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*30)\n",
        "    print(\"FINAL COMPARISON TABLE\")\n",
        "    print(\"=\"*30)\n",
        "    summary = pd.DataFrame(evaluations)\n",
        "    print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "57GN1zeJveiv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}